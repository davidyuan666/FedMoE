import numpy as np
import threading
import time
import random
import re
from typing import List, Dict, Tuple

# --- 模拟参数 ---
MODEL_DIM = 4096  # 基础 LLM 维度
LORA_RANK = 16    # LoRA 秩 (积木模型的“大小”)
SERVER_LR = 1.0   # 服务器端聚合学习率
STALENESS_DECAY = 0.1 # 异步更新的陈旧度衰减因子

# ----------------------------------------
# 1. 积木模型 (ExpertModel - The "Building Block")
# ----------------------------------------
class ExpertModel:
    """
    代表一个“积木模型” (或 LoRA 专家)。
    它只包含轻量级的 LoRA 权重。
    """
    def __init__(self, name, model_dim, lora_rank):
        self.name = name
        self.version = 0
        # 模拟 LoRA 权重 (A 和 B 矩阵)
        self.lora_A = np.zeros((model_dim, lora_rank))
        self.lora_B = np.zeros((lora_rank, model_dim))
        print(f"  [Coordinator] Registered Expert: {self.name}")

    def simulate_inference(self, prompt_part: str) -> str:
        """
        模拟此专家生成代码。
        在真实场景中，这将调用 (BaseModel + LoRA_A * LoRA_B) 进行推理。
        """
        # 模拟权重的影响
        weight_norm = np.linalg.norm(self.lora_A)
        return f"\n--- [Code from {self.name.upper()} (v{self.version}, norm={weight_norm:.2f})] ---\n" \
               f"def generated_for_{self.name.lower()}(prompt='{prompt_part[:30]}...'):\n" \
               f"    # Logic generated by this expert\n    pass\n" \
               f"--- [End of {self.name.upper()}] ---\n"

# ----------------------------------------
# 2. 中央协调器 (Central Coordinator)
# ----------------------------------------
class CentralCoordinator:
    """
    负责异步聚合 *多个* 专家的梯度更新。
    """
    def __init__(self):
        # 核心：服务器现在管理一个“模型集合体”
        self.experts: Dict[str, ExpertModel] = {}
        self.lock = threading.Lock()
        print("Central Coordinator Initialized.")

    def register_expert(self, expert_name: str, model_dim: int, lora_rank: int):
        """
        注册一个新的“积木模型”
        """
        with self.lock:
            if expert_name not in self.experts:
                self.experts[expert_name] = ExpertModel(expert_name, model_dim, lora_rank)

    def get_expert_weights(self, expert_name: str) -> Tuple[np.ndarray, np.ndarray, int]:
        """
        Worker 拉取特定专家的最新权重。
        """
        with self.lock:
            if expert_name not in self.experts:
                raise ValueError(f"Expert {expert_name} not registered.")
            expert = self.experts[expert_name]
            return expert.lora_A.copy(), expert.lora_B.copy(), expert.version

    def push_expert_update(self, expert_name: str, lora_delta: Tuple[np.ndarray, np.ndarray], worker_model_version: int):
        """
        Worker 异步推送特定专家的“梯度更新”（LoRA 增量）。
        """
        with self.lock:
            if expert_name not in self.experts:
                return # 专家可能已被移除
            
            expert = self.experts[expert_name]
            
            # 1. 计算陈旧度 (Staleness)
            staleness = expert.version - worker_model_version
            decay_factor = 1.0 / (1.0 + STALENESS_DECAY * staleness)
            
            # 2. 应用更新 (梯度交换)
            delta_A, delta_B = lora_delta
            expert.lora_A += SERVER_LR * decay_factor * delta_A
            expert.lora_B += SERVER_LR * decay_factor * delta_B
            
            # 3. 更新专家版本
            expert.version += 1
            
            print(f"[Coordinator] Updated {expert.name.upper()} (v{expert.version}) "
                  f"from Worker (Staleness: {staleness}, Decay: {decay_factor:.2f})")

# ----------------------------------------
# 3. 异构工作站 (Heterogeneous Worker)
# ----------------------------------------
class HeterogeneousWorker:
    """
    模拟一个异构的 GPU 机器。
    它有自己的“专长”（specialty）和“性能”（speed_factor）。
    """
    def __init__(self, worker_id: str, coordinator: CentralCoordinator, specialty: str, speed_factor: float):
        self.worker_id = worker_id
        self.coordinator = coordinator
        self.specialty = specialty  # 'python_expert', 'sql_expert', 'docs_expert'
        self.speed_factor = speed_factor # 1.0 = 正常, 0.5 = 快2倍, 2.0 = 慢2倍
        
        # 模拟该机器上的专用数据集
        self.local_data = f"Simulated local data for {specialty}"
        print(f"Worker {self.worker_id} (Specialty: {self.specialty}, Speed: {self.speed_factor:.1f}x) initialized.")

    def simulate_local_training(self, lora_A, lora_B):
        """
        模拟在本地*异构 GPU*上训练*特定数据*。
        """
        train_time = random.uniform(2, 5) * self.speed_factor
        print(f"  [{self.worker_id}] Starting training on {self.specialty}... (Est. {train_time:.2f}s)")
        time.sleep(train_time)
        
        # 模拟本地梯度更新
        # 真实场景中，这是在本地数据上训练 LoRA 的结果
        delta_A = np.random.randn(*lora_A.shape) * 0.05
        delta_B = np.random.randn(*lora_B.shape) * 0.05
        
        print(f"  [{self.worker_id}] Training complete.")
        return (delta_A, delta_B)

    def run_loop(self, stop_event):
        """
        Worker 的主循环：拉取 -> 训练 -> 推送
        """
        while not stop_event.is_set():
            try:
                # 1. 拉取 (Pull) - 只拉取自己专长的模型
                (global_A, global_B, global_version) = self.coordinator.get_expert_weights(self.specialty)
                
                # 2. 训练 (Train)
                lora_delta = self.simulate_local_training(global_A, global_B)
                
                # 3. 推送 (Push)
                network_delay = random.uniform(0.5, 2.0)
                time.sleep(network_delay) # 模拟网络延迟
                
                self.coordinator.push_expert_update(
                    self.specialty, 
                    lora_delta, 
                    global_version
                )
                
                # 模拟任务间隔
                time.sleep(random.uniform(3, 8))
                
            except Exception as e:
                print(f"[{self.worker_id}] Error: {e}")
                time.sleep(5)
        
        print(f"[{self.worker_id}] Shutting down.")

# ----------------------------------------
# 4. 推理智能体 (Inference Agent)
# ----------------------------------------
class InferenceAgent:
    """
    负责在推理时“协作”不同的积木模型。
    """
    def __init__(self, coordinator: CentralCoordinator):
        self.coordinator = coordinator
        print("\nInference Agent Initialized. Ready to route tasks.")

    def route_task(self, prompt: str) -> List[str]:
        """
        核心路由逻辑：根据 prompt 决定需要哪些专家。
        """
        prompt_lower = prompt.lower()
        required_experts = []
        
        if re.search(r'python|pandas|numpy|def ', prompt_lower):
            required_experts.append('python_expert')
            
        if re.search(r'sql|database|query|select', prompt_lower):
            required_experts.append('sql_expert')
            
        if re.search(r'docstring|explain|comment|how to', prompt_lower):
            required_experts.append('docs_expert')
            
        if not required_experts:
            # 默认回退
            required_experts.append('python_expert')
            
        print(f"[Agent] Routing prompt to: {required_experts}")
        return required_experts

    def generate_code(self, prompt: str) -> str:
        """
        执行“协作推理”。
        """
        print(f"\n[Agent] Received new task: '{prompt}'")
        
        # 1. 决定需要哪些专家
        required_experts = self.route_task(prompt)
        
        final_code_parts = []
        final_code_parts.append(f"# Task: {prompt}\n")
        final_code_parts.append(f"# Agent decided to use: {', '.join(required_experts)}\n" + ("-"*30))
        
        # 2. 协作：依次调用每个专家
        # (在更高级的 Agent 中，这里可能是并行调用或链式调用)
        for expert_name in required_experts:
            try:
                with self.coordinator.lock: # 确保在推理时模型不会被修改
                    expert = self.coordinator.experts[expert_name]
                
                # 模拟将任务的特定部分交给专家
                prompt_part = f"Generate {expert_name} part for: {prompt}"
                code_part = expert.simulate_inference(prompt_part)
                final_code_parts.append(code_part)
                
            except Exception as e:
                final_code_parts.append(f"\n--- [Error calling {expert_name}: {e}] ---\n")
                
        # 3. 组装成最终模型
        return "\n".join(final_code_parts)

# ----------------------------------------
# 5. 运行模拟 (Main Function)
# ----------------------------------------

if __name__ == "__main__":
    
    # 1. 初始化中央协调器
    coordinator = CentralCoordinator()
    
    # 2. 注册“积木模型” (代码生成场景)
    coordinator.register_expert('python_expert', MODEL_DIM, LORA_RANK)
    coordinator.register_expert('sql_expert', MODEL_DIM, LORA_RANK)
    coordinator.register_expert('docs_expert', MODEL_DIM, LORA_RANK)

    # 3. 初始化异构工作站
    # (注意：性能不一样，专长也不一样)
    workers = [
        HeterogeneousWorker("Worker-1-Fast", coordinator, 'python_expert', speed_factor=0.5), # 快的 Python 机器
        HeterogeneousWorker("Worker-2-Slow", coordinator, 'python_expert', speed_factor=2.0), # 慢的 Python 机器
        HeterogeneousWorker("Worker-3-SQL", coordinator, 'sql_expert', speed_factor=1.0),
        HeterogeneousWorker("Worker-4-Docs", coordinator, 'docs_expert', speed_factor=1.5),
        HeterogeneousWorker("Worker-5-Python", coordinator, 'python_expert', speed_factor=1.0),
    ]

    # 4. 启动分布式训练 (在后台线程)
    print("\n--- [Phase 1: Starting Distributed Training] ---")
    stop_event = threading.Event()
    threads = []
    for worker in workers:
        t = threading.Thread(target=worker.run_loop, args=(stop_event,))
        t.start()
        threads.append(t)

    # 5. 让训练运行 15 秒钟
    time.sleep(15)
    
    # 6. 停止训练
    print("\n--- [Phase 2: Stopping Training] ---")
    stop_event.set()
    for t in threads:
        t.join(timeout=3.0) # 等待线程退出
    print("All workers stopped.")

    # 7. 启动 Agent 进行协作推理
    print("\n--- [Phase 3: Starting Inference Agent] ---")
    agent = InferenceAgent(coordinator)
    
    # 任务1: 纯 Python
    code1 = agent.generate_code("Write a python function to sort a list")
    print(code1)

    # 任务2: 协作 (Python + SQL)
    code2 = agent.generate_code("Write a python script to query the 'users' database")
    print(code2)
    
    # 任务3: 协作 (Python + Docs)
    code3 = agent.generate_code("How do I use the new python function 'calculate_metrics'?")
    print(code3)